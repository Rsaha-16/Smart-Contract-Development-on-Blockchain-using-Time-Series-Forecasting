{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from darts import TimeSeries\n",
    "from darts.models import TransformerModel\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable CUDA by setting the environment variable\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "# Force PyTorch to use CPU only\n",
    "torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochEndCallback(Callback):\n",
    "    def __init__(self, log_file='epoch_loss_log_100ep_RSI_Vol.txt'):\n",
    "        self.log_file = log_file\n",
    "        with open(self.log_file, 'w') as f:\n",
    "            f.write('Epoch,Loss\\n')\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        train_loss = trainer.callback_metrics.get(\"train_loss\")\n",
    "        current_epoch = trainer.current_epoch\n",
    "        with open(self.log_file, 'a') as f:\n",
    "            f.write(f'{current_epoch},{train_loss},\\n')\n",
    "        print(f\"Epoch {current_epoch} ended with training loss: {train_loss}\")\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        val_loss = trainer.callback_metrics.get(\"val_loss\")\n",
    "        current_epoch = trainer.current_epoch\n",
    "        with open(self.log_file, 'a') as f:\n",
    "            f.seek(0, 2)\n",
    "            f.seek(f.tell() - 2, 0)\n",
    "            f.write(f'{val_loss}\\n')\n",
    "        print(f\"Epoch {current_epoch} ended with validation loss: {val_loss}\")\n",
    "\n",
    "# Create an instance of the callback\n",
    "epoch_end_callback = EpochEndCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the early stopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='train_loss',  # Monitor the training loss\n",
    "    patience=10,           # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=True,          # Print messages when early stopping is triggered\n",
    "    mode='min'             # Minimize the monitored metric\n",
    ")\n",
    "\n",
    "# Pass the callback through pl_trainer_kwargs\n",
    "pl_trainer_kwargs = {\n",
    "    \"callbacks\": [epoch_end_callback, early_stopping],  # Include both callbacks\n",
    "    \"accelerator\": \"cpu\",  # Force CPU usage\n",
    "    \"devices\": 1  # Use a single CPU\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the data from the CSV file\n",
    "csv_file_path = '/home/raj/Rajarshi/Term Project/Sir_code_dart/data/SBIN.NS_day_2022.csv'\n",
    "sbi_data = pd.read_csv(csv_file_path, parse_dates=['Date'])\n",
    "\n",
    "# Drop rows with any NaN values\n",
    "sbi_data.dropna(inplace=True)\n",
    "\n",
    "# Ensure 'Date' is set as the index\n",
    "sbi_data.set_index('Date', inplace=True)\n",
    "\n",
    "# Step 2: Detect missing dates and count them\n",
    "full_range = pd.date_range(start=sbi_data.index.min(), end=sbi_data.index.max(), freq='B')\n",
    "missing_dates = full_range.difference(sbi_data.index)\n",
    "print(f\"Number of missing dates: {len(missing_dates)}\")\n",
    "if len(missing_dates) > 0:\n",
    "    print(f\"Missing dates filled: {missing_dates}\")\n",
    "\n",
    "# Step 3: Fill missing dates and set frequency\n",
    "sbi_data = sbi_data.asfreq('B', method='pad')  # Fill missing dates with the previous value\n",
    "\n",
    "# Separate scalers for the target (Close) and the past covariates (RSI, MACD, Volume, Bollinger Bands)\n",
    "scaler_close = MinMaxScaler()\n",
    "scaler_covariates = MinMaxScaler()\n",
    "\n",
    "# Normalize the target and past covariates separately\n",
    "sbi_data['Close'] = scaler_close.fit_transform(sbi_data[['Close']])\n",
    "sbi_data[['RSI', 'MACD', 'Volume', 'Upper_Bollinger_Band', 'Middle_Bollinger_Band', 'Lower_Bollinger_Band']] = scaler_covariates.fit_transform(\n",
    "    sbi_data[['RSI', 'MACD', 'Volume', 'Upper_Bollinger_Band', 'Middle_Bollinger_Band', 'Lower_Bollinger_Band']]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the TimeSeries objects\n",
    "target = TimeSeries.from_series(sbi_data['Close'])\n",
    "past_cov = TimeSeries.from_dataframe(sbi_data[['RSI', 'MACD', 'Volume', 'Upper_Bollinger_Band', 'Middle_Bollinger_Band', 'Lower_Bollinger_Band']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configure and train the model\n",
    "model = TransformerModel(\n",
    "    input_chunk_length=60,\n",
    "    output_chunk_length=7,\n",
    "    d_model=128,  # Increased from 64\n",
    "    nhead=8,  # Increased from 4\n",
    "    num_encoder_layers=4,  # Increased from 3\n",
    "    num_decoder_layers=4,  # Increased from 3\n",
    "    dim_feedforward=1024,  # Increased from 512\n",
    "    dropout=0.2,  # Increased regularization\n",
    "    n_epochs=100,  # More epochs for better convergence\n",
    "    optimizer_cls=torch.optim.Adam,\n",
    "    optimizer_kwargs={'lr': 0.0005},\n",
    "    lr_scheduler_cls=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    lr_scheduler_kwargs={\n",
    "        'patience': 5,\n",
    "        'factor': 0.2,\n",
    "        'verbose': True,\n",
    "        'monitor': 'train_loss'  # Use train_loss if no validation set\n",
    "    },\n",
    "    pl_trainer_kwargs=pl_trainer_kwargs,\n",
    ")\n",
    "\n",
    "# Fit the model with the target series and past covariates\n",
    "model.fit(target, past_covariates=past_cov)\n",
    "\n",
    "# Predict the next 6 days after the last date in the dataset\n",
    "pred = model.predict(n=10, past_covariates=past_cov)\n",
    "\n",
    "# Denormalize the predictions using the close scaler\n",
    "pred_values = scaler_close.inverse_transform(pred.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the prediction dates\n",
    "dates = pred.time_index\n",
    "\n",
    "# Create a DataFrame to store the predictions along with the dates\n",
    "predictions_df = pd.DataFrame({\n",
    "    'Date': dates,\n",
    "    'Predicted Value': pred_values.flatten()\n",
    "})\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "output_csv_file = 'predicted_values_1_100ep_RSI_MACD_Vol_Bollinger.csv'\n",
    "predictions_df.to_csv(output_csv_file, index=False)\n",
    "\n",
    "# Print confirmation\n",
    "print(f\"Predicted values saved to {output_csv_file}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
