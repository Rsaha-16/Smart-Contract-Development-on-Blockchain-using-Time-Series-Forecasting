import pandas as pd
import numpy as np
from neuralforecast import NeuralForecast
from neuralforecast.models import iTransformer
from neuralforecast.losses.pytorch import MAE
from sklearn.preprocessing import MinMaxScaler
from sklearn.neural_network import MLPRegressor
import pytorch_lightning as pl

# Custom callback to save training losses
class SaveTrainingLossCallback(pl.Callback):
    def __init__(self, log_file='epoch_loss_log_ensemble_itransformer_model_4.txt'):
        self.training_losses = []
        self.log_file = log_file
        with open(self.log_file, 'w') as f:
            f.write('Epoch,Train_Loss\n')

    def on_train_epoch_end(self, trainer, pl_module):
        train_loss = trainer.callback_metrics['train_loss'].item()
        self.training_losses.append(train_loss)
        with open(self.log_file, 'a') as f:
            f.write(f'{trainer.current_epoch},{train_loss}\n')

# Initialize callbacks
save_loss_callback = SaveTrainingLossCallback()
pl_trainer_kwargs = {"callbacks": [save_loss_callback], "accelerator": "cpu", "devices": 1}

# Load and preprocess the data
csv_file_path = '/home/raj/Rajarshi/Term Project/rajarshi_code/rajarshi_code/data/SBIN.NS_day_2022.csv'
sbi_data = pd.read_csv(csv_file_path, parse_dates=['Date'])
sbi_data.dropna(inplace=True)
sbi_data.set_index('Date', inplace=True)
sbi_data = sbi_data.asfreq('B', method='pad')

# Create scalers
scaler_close = MinMaxScaler()
sbi_data['Open_Close_Diff'] = sbi_data['Open'] - sbi_data['Close']
sbi_data['Close'] = scaler_close.fit_transform(sbi_data[['Close']])

# Add technical indicators
# Moving Averages
sbi_data['MA_5'] = sbi_data['Close'].rolling(window=5).mean()
sbi_data['MA_20'] = sbi_data['Close'].rolling(window=20).mean()
sbi_data['MA_50'] = sbi_data['Close'].rolling(window=50).mean()

# Relative Strength Index (RSI)
delta = sbi_data['Close'].diff()
gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
rs = gain / loss
sbi_data['RSI'] = 100 - (100 / (1 + rs))

# MACD
sbi_data['EMA_12'] = sbi_data['Close'].ewm(span=12, adjust=False).mean()
sbi_data['EMA_26'] = sbi_data['Close'].ewm(span=26, adjust=False).mean()
sbi_data['MACD'] = sbi_data['EMA_12'] - sbi_data['EMA_26']
sbi_data['MACD_Signal'] = sbi_data['MACD'].ewm(span=9, adjust=False).mean()

# Bollinger Bands
sbi_data['BB_Upper'] = sbi_data['MA_20'] + 2 * sbi_data['Close'].rolling(window=20).std()
sbi_data['BB_Lower'] = sbi_data['MA_20'] - 2 * sbi_data['Close'].rolling(window=20).std()

# ADX
sbi_data['High_Low'] = sbi_data['High'] - sbi_data['Low']
sbi_data['High_Close'] = (sbi_data['High'] - sbi_data['Close'].shift()).abs()
sbi_data['Low_Close'] = (sbi_data['Low'] - sbi_data['Close'].shift()).abs()
sbi_data['True_Range'] = sbi_data[['High_Low', 'High_Close', 'Low_Close']].max(axis=1)
sbi_data['DX'] = (sbi_data['True_Range'].rolling(window=14).mean() / sbi_data['True_Range'].rolling(window=14).mean()) * 100
sbi_data['ADX'] = sbi_data['DX'].rolling(window=14).mean()

# Drop NaN values generated by rolling calculations
sbi_data.dropna(inplace=True)

# Prepare the dataset for training
Y_train_df = sbi_data.reset_index().rename(columns={'Date': 'ds', 'Close': 'y'})
Y_train_df['unique_id'] = 'SBIN'

# Define and train two iTransformer models with different hyperparameters
model_1 = iTransformer(
    h=7, input_size=60, n_series=1, hidden_size=512, n_heads=8,
    e_layers=2, d_layers=1, d_ff=2048, factor=1, dropout=0.1, use_norm=True,
    loss=MAE(), learning_rate=0.0009, max_steps=1000, **{'callbacks': [save_loss_callback]}
)

model_2 = iTransformer(
    h=7, input_size=60, n_series=1, hidden_size=512, n_heads=8,
    e_layers=2, d_layers=1, d_ff=2048, factor=1, dropout=0.1, use_norm=True,
    loss=MAE(), learning_rate=0.001, max_steps=1000, **{'callbacks': [save_loss_callback]}
)

# Create NeuralForecast object for each model and fit them
nf_model_1 = NeuralForecast(models=[model_1], freq='B')
nf_model_2 = NeuralForecast(models=[model_2], freq='B')

nf_model_1.fit(df=Y_train_df)
nf_model_2.fit(df=Y_train_df)

# Generate future dataframe for predictions
futr_df = nf_model_1.make_future_dataframe()

# Predict using both models
forecasts_model_1 = nf_model_1.predict(futr_df=futr_df)
forecasts_model_2 = nf_model_2.predict(futr_df=futr_df)

# Rescale the predictions back to original scale
pred_values_model_1 = scaler_close.inverse_transform(forecasts_model_1[['iTransformer']].values)
pred_values_model_2 = scaler_close.inverse_transform(forecasts_model_2[['iTransformer']].values)
dates = futr_df['ds']

# Stack the predictions from both models
stacked_predictions = np.hstack((pred_values_model_1, pred_values_model_2))

# Prepare the target values for MLP training (consider the last known true values)
# Use the actual values of 'y' for the last 7 days (for example)
true_values = scaler_close.inverse_transform(Y_train_df['y'][-7:].values.reshape(-1, 1)).flatten()

# Train an MLP to combine the predictions
mlp = MLPRegressor(hidden_layer_sizes=(10, 10), activation='relu', solver='adam', max_iter=1000, random_state=42)
mlp.fit(stacked_predictions, true_values)

# Predict using MLP
final_predictions = mlp.predict(stacked_predictions)

# Save the final predictions with corresponding dates
predictions_df = pd.DataFrame({'Date': dates, 'Predicted Value': final_predictions})
output_csv_file = 'ensemble_prediction_itransformer_model_4.csv'
predictions_df.to_csv(output_csv_file, index=False)

print(f"Ensemble Predictions saved to {output_csv_file}")

# Print the logged training losses for inspection
print("Training Losses:", save_loss_callback.training_losses)
